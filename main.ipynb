{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8994037,"sourceType":"datasetVersion","datasetId":5417429}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install tiktoken blobfile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T11:11:06.830617Z","iopub.execute_input":"2024-09-14T11:11:06.830899Z","iopub.status.idle":"2024-09-14T11:11:22.602545Z","shell.execute_reply.started":"2024-09-14T11:11:06.830866Z","shell.execute_reply":"2024-09-14T11:11:22.601443Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting blobfile\n  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nCollecting pycryptodomex>=3.8 (from blobfile)\n  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: urllib3<3,>=1.25.3 in /opt/conda/lib/python3.10/site-packages (from blobfile) (1.26.18)\nRequirement already satisfied: lxml>=4.9 in /opt/conda/lib/python3.10/site-packages (from blobfile) (5.3.0)\nRequirement already satisfied: filelock>=3.0 in /opt/conda/lib/python3.10/site-packages (from blobfile) (3.15.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\nDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycryptodomex, tiktoken, blobfile\nSuccessfully installed blobfile-3.0.0 pycryptodomex-3.20.0 tiktoken-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.set_default_tensor_type(torch.BFloat16Tensor) # Otherwise it may not fit into memory.","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:11:22.604988Z","iopub.execute_input":"2024-09-14T11:11:22.605420Z","iopub.status.idle":"2024-09-14T11:11:25.648079Z","shell.execute_reply.started":"2024-09-14T11:11:22.605354Z","shell.execute_reply":"2024-09-14T11:11:25.646951Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /usr/local/src/pytorch/torch/csrc/tensor/python_tensor.cpp:432.)\n  _C._set_default_tensor_type(t)\n","output_type":"stream"}]},{"cell_type":"code","source":"DIM = 4096 # Llama3 Table 3.\nFFN_DIM = 14336 # For POSITION-WISE FEED-FORWARD NETWORK (FFN) Llama Table 3\nN_LAYERS = 32 # Llama3 Table 3.\nN_HEADS = 32 # Llama3 Table 3.\nN_KV_HEADS = 8 # With 8 key-value heads to improve inference speed and to reduce the size (llama3)\nVOCAB_SIZE = 128256 # Llama3 vocab size. Llama3 paper says 128K. This is the exact number taken from tokenizer.\nNORM_EPS = 1e-5 # Took from Llama3 code ModelArgs.\nROPE_THETA = 500000 # We increase the RoPE base frequency hyperparameter to 500,000 (llama3)\nMAX_BATCH_SIZE = 4 # Just optional depending on your specs. If number of examples you provide is smaller, it takes it as batch size.\nMAX_SEQ_LEN = 128 # Just optional depending on your specs.\nN_KV_HEAD_REP = N_HEADS // N_KV_HEADS # How many times you repeat KV to match your queries(N_HEADS).\nHEAD_DIM = DIM // N_HEADS # Divide dimension by number of heads to get dimension per head.","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:11:25.649831Z","iopub.execute_input":"2024-09-14T11:11:25.650358Z","iopub.status.idle":"2024-09-14T11:11:25.656441Z","shell.execute_reply.started":"2024-09-14T11:11:25.650298Z","shell.execute_reply":"2024-09-14T11:11:25.655378Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Apply pre-normalization using RMSNorm (llama2)\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim, norm_eps):\n        super().__init__()\n        self.norm_eps = norm_eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n    \n    def forward(self, x):\n        out = self._norm(x.float()).type_as(x)\n        return out * self.weight # (2, 8, DIM) Values stays the same. We make the tensor grad_fn.\n    \ndummy_inp = torch.randn(2, 8, DIM)\nnorm = RMSNorm(DIM, NORM_EPS)\noutput = norm(dummy_inp)\nprint(output.shape)\n\ndef precompute_freqs_cis(dim, end, theta = 10000.0):\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis\n\n\ndef reshape_for_broadcast(freqs_cis, x):\n    ndim = x.ndim\n    assert 0 <= 1 < ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(*shape)\n\n\ndef apply_rotary_emb(xq, xk, freqs_cis):\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\ndummy_inp1 = torch.randn(2, 8, N_HEADS, HEAD_DIM)\ndummy_inp2 = torch.randn(2, 8, N_KV_HEADS, HEAD_DIM)\n\ndummy_freqs_cis = precompute_freqs_cis(HEAD_DIM, MAX_SEQ_LEN*2, ROPE_THETA)\ndummy_freqs_cis = dummy_freqs_cis[0 : 0 + 8]\n\nout1, out2 = apply_rotary_emb(dummy_inp1, dummy_inp2, dummy_freqs_cis)\nprint(\"-\"*30)\nprint(out1.shape)\nprint(out2.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:11:25.659211Z","iopub.execute_input":"2024-09-14T11:11:25.659602Z","iopub.status.idle":"2024-09-14T11:11:25.794218Z","shell.execute_reply.started":"2024-09-14T11:11:25.659558Z","shell.execute_reply":"2024-09-14T11:11:25.792424Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"torch.Size([2, 8, 4096])\n------------------------------\ntorch.Size([2, 8, 32, 128])\ntorch.Size([2, 8, 8, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Bias is false. It usually adds overhead to the transformer models.\n        self.w1 = nn.Linear(DIM, FFN_DIM, bias=False)\n        self.w3 = nn.Linear(DIM, FFN_DIM, bias=False)\n        self.w2 = nn.Linear(FFN_DIM, DIM, bias=False)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x)) # (2, 8, DIM) = (bsz, seqlen, DIM) - use the SwiGLU activation function (llama3) Table 3.\n    \ndummy_inp = torch.randn(2, 8, DIM)\n\nfeed_forward = FeedForward()\n\noutput = feed_forward(dummy_inp)\ndel feed_forward\nprint(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:11:25.795604Z","iopub.execute_input":"2024-09-14T11:11:25.795979Z","iopub.status.idle":"2024-09-14T11:11:27.812286Z","shell.execute_reply.started":"2024-09-14T11:11:25.795938Z","shell.execute_reply":"2024-09-14T11:11:27.811427Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([2, 8, 4096])\n","output_type":"stream"}]},{"cell_type":"code","source":"# GQA With Cache\nclass Attention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wq = nn.Linear(DIM, N_HEADS * HEAD_DIM, bias=False)\n        self.wk = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False)\n        self.wv = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False)\n        self.wo = nn.Linear(N_HEADS * HEAD_DIM, DIM, bias=False) # Weight matrix defined in the MultiheadAttention formula.\n\n        # Create empty caches for keys and values.\n        self.cache_k = torch.zeros(\n            (\n                MAX_BATCH_SIZE,\n                MAX_SEQ_LEN,\n                N_KV_HEADS,\n                HEAD_DIM,\n            )\n        )\n        self.cache_v = torch.zeros(\n            (\n                MAX_BATCH_SIZE,\n                MAX_SEQ_LEN,\n                N_KV_HEADS,\n                HEAD_DIM,\n            )\n        )\n\n    def forward(self, x, start_pos, freqs_cis, mask):\n        bsz, seqlen, _ = x.shape # Get batch size and sequence length. (bsz, seqlen, DIM)\n        queries, keys, values = self.wq(x), self.wk(x), self.wv(x) # q -> (bsz, seqlen, N_HEADS*HEAD_DIM) | k,v -> (bsz, seqlen, N_KV_HEADS*HEAD_DIM)\n\n        queries = queries.view(bsz, seqlen, N_HEADS, HEAD_DIM)\n        keys = keys.view(bsz, seqlen, N_KV_HEADS, HEAD_DIM)\n        values = values.view(bsz, seqlen, N_KV_HEADS, HEAD_DIM)\n\n        queries, keys = apply_rotary_emb(queries, keys, freqs_cis=freqs_cis)\n\n        self.cache_k = self.cache_k.to(queries.device)\n        self.cache_v = self.cache_v.to(queries.device)\n\n        self.cache_k[:bsz, start_pos : start_pos + seqlen] = keys\n        self.cache_v[:bsz, start_pos : start_pos + seqlen] = values\n\n        keys = self.cache_k[:bsz, : start_pos + seqlen]\n        values = self.cache_v[:bsz, : start_pos + seqlen]\n\n        # In these runs we simply duplicated the KV heads for MQA in all GPUs. (llama2)\n        keys = torch.repeat_interleave(\n            keys, dim=2, repeats=N_KV_HEAD_REP\n        ) # (bsz, seqlen, N_KV_HEADS, HEAD_DIM) -> (bsz, seqlen, N_HEADS, HEAD_DIM)\n        values = torch.repeat_interleave(\n            values, dim=2, repeats=N_KV_HEAD_REP\n        )  # (bsz, seqlen, N_KV_HEADS, HEAD_DIM) -> (bsz, seqlen, N_HEADS, HEAD_DIM)\n\n        # Reshaping for scaled_dot_product_attention. (bsz, ..., seqlen, HEAD_DIM) expected.\n        queries = queries.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n        keys = keys.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n        values = values.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n\n        out = F.scaled_dot_product_attention(\n            queries,\n            keys,\n            values,\n            attn_mask=mask,\n        ) # (bsz, N_HEADS, seqlen, HEAD_DIM)\n        \n        \n        # If we don't use `contiguous` torch may complain.\n        out = out.transpose(1, 2).contiguous().view(bsz, seqlen, -1) # transpose, (bsz, seqlen, N_HEADS, HEAD_DIM) - (bsz, seqlen, DIM) - -1 does N_HEAD * HEAD_DIM = DIM\n        return self.wo(out) # (bsz, seqlen, DIM)\n    \ndummy_inp = torch.randn(2, 8, DIM) # 8 is sequence length. Depends on your input. I put 8.\ndummy_start_pos = 0\ndummy_freqs_cis = torch.randn(8, 64)\ndummy_mask = torch.randn(8, 8) # Mask is size (seqlen, seqlen)\n\nattention = Attention()\n\noutput = attention(dummy_inp, dummy_start_pos, dummy_freqs_cis, dummy_mask)\nprint(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)\ndel attention","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:11:27.813978Z","iopub.execute_input":"2024-09-14T11:11:27.814723Z","iopub.status.idle":"2024-09-14T11:11:28.727891Z","shell.execute_reply.started":"2024-09-14T11:11:27.814678Z","shell.execute_reply":"2024-09-14T11:11:28.727010Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"torch.Size([2, 8, 4096])\n","output_type":"stream"}]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.attention = Attention()\n        self.feed_forward = FeedForward()\n        self.attention_norm = RMSNorm(DIM, NORM_EPS)\n        self.ffn_norm = RMSNorm(DIM, NORM_EPS)\n\n    def forward(self, x, start_pos, freqs_cis, mask):\n        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask) # (2, 8, 4096) = (bsz, seqlen, DIM)\n        out = h + self.feed_forward(self.ffn_norm(h)) # (2, 8, DIM) = (bsz, seqlen, DIM)\n        return out # (2, 8, DIM) = (bsz, seqlen, DIM)\n    \ndummy_inp = torch.randn(2, 8, DIM)\ndummy_start_pos = 0\ndummy_freqs_cis = torch.randn(8, 64)\ndummy_mask = torch.randn(8, 8)\n\ntransformer_block = TransformerBlock()\n\noutput = transformer_block(dummy_inp, dummy_start_pos, dummy_freqs_cis, dummy_mask)\nprint(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:11:28.729478Z","iopub.execute_input":"2024-09-14T11:11:28.730185Z","iopub.status.idle":"2024-09-14T11:11:31.398547Z","shell.execute_reply.started":"2024-09-14T11:11:28.730142Z","shell.execute_reply":"2024-09-14T11:11:31.397556Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([2, 8, 4096])\n","output_type":"stream"}]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tok_embeddings = nn.Embedding(\n            VOCAB_SIZE, DIM\n        )\n        \n        self.layers = torch.nn.ModuleList()\n        for _ in range(N_LAYERS):\n            self.layers.append(TransformerBlock())\n\n        self.norm = RMSNorm(DIM, NORM_EPS)\n        self.output = nn.Linear(DIM, VOCAB_SIZE, bias=False,)\n\n        self.freqs_cis = precompute_freqs_cis(\n            HEAD_DIM,\n            MAX_SEQ_LEN * 2,\n            ROPE_THETA,\n        )\n\n    @torch.inference_mode()\n    def forward(self, tokens, start_pos):       \n        _bsz, seqlen = tokens.shape\n        h = self.tok_embeddings(tokens) # (bsz, seqlen, DIM)\n        self.freqs_cis = self.freqs_cis.to(tokens.device)\n        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n\n        mask = None # When we take the tokens from the cached values (seqlen=1) we don't need any aditional mask.\n        if seqlen > 1: # Because of KV Cache, we process only 1 token. However, the first run doesn't have any cache. So it has a seqlen > 1.\n            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device) # Since this is the first pass, we don't have any KV Cache. So we need a mask. Create (seqlen, seqlen) matrix with float(\"-inf\") values.\n\n            mask = torch.triu(mask, diagonal=1).to(tokens.device) # Take the upper triangle excluding diagonal since it's casual LM.\n\n        for layer in self.layers:\n            h = layer(h, start_pos, freqs_cis, mask) # (2, 8, 4096) = (bsz, seqlen, DIM)\n        h = self.norm(h) # (2, 8, 4096) = (bsz, seqlen, DIM)\n        out = self.output(h).float() # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n        return out # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n    \ndummy_tokens = torch.rand(2, 8).long() # Use `rand` instead of `randn`. `randn` generates negative number which is invalid for `nn.Embedding`\ndummy_start_pos = 0\n\ntransformer = Transformer()\n\noutput = transformer(dummy_tokens, dummy_start_pos)\nprint(output.shape) # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\ndel transformer","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:11:31.399968Z","iopub.execute_input":"2024-09-14T11:11:31.400608Z","iopub.status.idle":"2024-09-14T11:13:05.999896Z","shell.execute_reply.started":"2024-09-14T11:11:31.400553Z","shell.execute_reply":"2024-09-14T11:13:05.998799Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"torch.Size([2, 8, 128256])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n\nimport os\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import (\n    AbstractSet,\n    cast,\n    Collection,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Sequence,\n    TypedDict,\n    Union,\n)\n\n\nlogger = getLogger(__name__)\nimport tiktoken\nfrom tiktoken.load import load_tiktoken_bpe\n\nRole = Literal[\"system\", \"user\", \"assistant\"]\n\n\nclass Message(TypedDict):\n    role: Role\n    content: str\n\n\nDialog = Sequence[Message]\n\n\nclass Tokenizer:\n    \"\"\"\n    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n    \"\"\"\n\n    special_tokens: Dict[str, int]\n\n    num_reserved_special_tokens = 256\n\n    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n\n    def __init__(self, model_path: str):\n        \"\"\"\n        Initializes the Tokenizer with a Tiktoken model.\n\n        Args:\n            model_path (str): The path to the Tiktoken model file.\n        \"\"\"\n        assert os.path.isfile(model_path), model_path\n\n        mergeable_ranks = load_tiktoken_bpe(model_path)\n        num_base_tokens = len(mergeable_ranks)\n        special_tokens = [\n            \"<|begin_of_text|>\",\n            \"<|end_of_text|>\",\n            \"<|reserved_special_token_0|>\",\n            \"<|reserved_special_token_1|>\",\n            \"<|reserved_special_token_2|>\",\n            \"<|reserved_special_token_3|>\",\n            \"<|start_header_id|>\",\n            \"<|end_header_id|>\",\n            \"<|reserved_special_token_4|>\",\n            \"<|eot_id|>\",  # end of turn\n        ] + [\n            f\"<|reserved_special_token_{i}|>\"\n            for i in range(5, self.num_reserved_special_tokens - 5)\n        ]\n        self.special_tokens = {\n            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n        }\n        self.model = tiktoken.Encoding(\n            name=Path(model_path).name,\n            pat_str=self.pat_str,\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=self.special_tokens,\n        )\n        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n\n        self.n_words: int = self.model.n_vocab\n        # BOS / EOS token IDs\n        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n        self.pad_id: int = -1\n        self.stop_tokens = {\n            self.special_tokens[\"<|end_of_text|>\"],\n            self.special_tokens[\"<|eot_id|>\"],\n        }\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n\n    def encode(\n        self,\n        s: str,\n        *,\n        bos: bool,\n        eos: bool,\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n    ) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            s (str): The input string to be encoded.\n            bos (bool): Whether to prepend the beginning-of-sequence token.\n            eos (bool): Whether to append the end-of-sequence token.\n            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n\n        Returns:\n            list[int]: A list of token IDs.\n\n        By default, setting disallowed_special=() encodes a string by ignoring\n        special tokens. Specifically:\n        - Setting `disallowed_special` to () will cause all text corresponding\n          to special tokens to be encoded as natural text (insteading of raising\n          an error).\n        - Setting `allowed_special` to \"all\" will treat all text corresponding\n          to special tokens to be encoded as special tokens.\n        \"\"\"\n        assert type(s) is str\n\n        # The tiktoken tokenizer can handle <=400k chars without\n        # pyo3_runtime.PanicException.\n        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n\n        # https://github.com/openai/tiktoken/issues/195\n        # Here we iterate over subsequences and split if we exceed the limit\n        # of max consecutive non-whitespace or whitespace characters.\n        MAX_NO_WHITESPACES_CHARS = 25_000\n\n        substrs = (\n            substr\n            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n            for substr in self._split_whitespaces_or_nonwhitespaces(\n                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n            )\n        )\n        t: List[int] = []\n        for substr in substrs:\n            t.extend(\n                self.model.encode(\n                    substr,\n                    allowed_special=allowed_special,\n                    disallowed_special=disallowed_special,\n                )\n            )\n        if bos:\n            t.insert(0, self.bos_id)\n        if eos:\n            t.append(self.eos_id)\n        return t\n\n    def decode(self, t: Sequence[int]) -> str:\n        \"\"\"\n        Decodes a list of token IDs into a string.\n\n        Args:\n            t (List[int]): The list of token IDs to be decoded.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n        return self.model.decode(cast(List[int], t))\n\n    @staticmethod\n    def _split_whitespaces_or_nonwhitespaces(\n        s: str, max_consecutive_slice_len: int\n    ) -> Iterator[str]:\n        \"\"\"\n        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n        consecutive whitespaces or consecutive non-whitespaces.\n        \"\"\"\n        current_slice_len = 0\n        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n        slice_start = 0\n\n        for i in range(len(s)):\n            is_now_space = s[i].isspace()\n\n            if current_slice_is_space ^ is_now_space:\n                current_slice_len = 1\n                current_slice_is_space = is_now_space\n            else:\n                current_slice_len += 1\n                if current_slice_len > max_consecutive_slice_len:\n                    yield s[slice_start:i]\n                    slice_start = i\n                    current_slice_len = 1\n        yield s[slice_start:]\n\n\nclass ChatFormat:\n    def __init__(self, tokenizer: Tokenizer):\n        self.tokenizer = tokenizer\n\n    def encode_header(self, message: Message) -> List[int]:\n        tokens = []\n        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n        return tokens\n\n    def encode_message(self, message: Message) -> List[int]:\n        tokens = self.encode_header(message)\n        tokens.extend(\n            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n        )\n        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n        return tokens\n\n    def encode_dialog_prompt(self, dialog: Dialog) -> List[int]:\n        tokens = []\n        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n        for message in dialog:\n            tokens.extend(self.encode_message(message))\n        # Add the start of an assistant message for the model to complete.\n        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n        return tokens\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, TypedDict\n\nimport torch\nimport torch.nn.functional as F\n\n\nclass CompletionPrediction(TypedDict, total=False):\n    generation: str\n    tokens: List[str]  # not required\n    logprobs: List[float]  # not required\n\n\nclass ChatPrediction(TypedDict, total=False):\n    generation: Message\n    tokens: List[str]  # not required\n    logprobs: List[float]  # not required\n\n\nclass Llama:\n    @staticmethod\n    def build(\n        ckpt_dir: str,\n        tokenizer_path: str,\n        max_seq_len: int,\n        max_batch_size: int,\n        model_parallel_size: Optional[int] = None,\n        seed: int = 1,\n    ) -> \"Llama\":\n        \"\"\"\n        Build a Llama instance by initializing and loading a model checkpoint.\n\n        Args:\n            ckpt_dir (str): Path to the directory containing checkpoint files.\n            tokenizer_path (str): Path to the tokenizer file.\n            max_seq_len (int): Maximum sequence length for input text.\n            max_batch_size (int): Maximum batch size for inference.\n            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n                If not provided, it's determined from the environment. Defaults to None.\n\n        Returns:\n            Llama: An instance of the Llama class with the loaded model and tokenizer.\n\n        Raises:\n            AssertionError: If there are no checkpoint files in the specified directory,\n                or if the model parallel size does not match the number of checkpoint files.\n\n        Note:\n            This method initializes the distributed process group, sets the device to CUDA,\n            and loads the pre-trained model and tokenizer.\n        \"\"\"\n        assert 1 <= max_seq_len <= 8192, f\"max_seq_len must be between 1 and 8192, got {max_seq_len}.\"\n        assert os.path.isdir(ckpt_dir), f\"Checkpoint directory '{ckpt_dir}' does not exist.\"\n        assert os.path.isfile(tokenizer_path), f\"Tokenizer file '{tokenizer_path}' does not exist.\"\n\n        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        torch.cuda.set_device(local_rank)\n\n        # seed must be the same in all processes\n        torch.manual_seed(seed)\n\n        if local_rank > 0:\n            sys.stdout = open(os.devnull, \"w\")\n\n        start_time = time.time()\n        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n        checkpoint = torch.load(ckpt_dir+\"consolidated.00.pth\", map_location=\"cpu\")\n        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n            params = json.loads(f.read())\n            \n        tokenizer = Tokenizer(model_path=tokenizer_path)\n        assert VOCAB_SIZE == tokenizer.n_words\n        if torch.cuda.is_bf16_supported():\n            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n        else:\n            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n        model = Transformer()\n        print(f\"PARAMETERS: {sum(p.numel() for p in model.parameters())}\")\n        model.load_state_dict(checkpoint, strict=False)\n        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n\n        return Llama(model, tokenizer)\n\n    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.formatter = ChatFormat(tokenizer)\n\n    @torch.inference_mode()\n    def generate(\n        self,\n        prompt_tokens: List[List[int]],\n        max_gen_len: int,\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        logprobs: bool = False,\n        echo: bool = False,\n    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n        \"\"\"\n        Generate text sequences based on provided prompts using the language generation model.\n\n        Args:\n            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n            max_gen_len (int): Maximum length of the generated text sequence.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\n        Returns:\n            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n\n        Note:\n            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n            If logprobs is True, token log probabilities are computed for each generated token.\n\n        \"\"\"\n        bsz = len(prompt_tokens)\n        assert bsz <= MAX_BATCH_SIZE, (bsz, MAX_BATCH_SIZE)\n\n        min_prompt_len = min(len(t) for t in prompt_tokens)\n        max_prompt_len = max(len(t) for t in prompt_tokens)\n        assert max_prompt_len <= MAX_SEQ_LEN\n        total_len = min(MAX_SEQ_LEN, max_gen_len + max_prompt_len)\n\n        pad_id = self.tokenizer.pad_id\n        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n        for k, t in enumerate(prompt_tokens):\n            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n        if logprobs:\n            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n\n        prev_pos = 0\n        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n        input_text_mask = tokens != pad_id\n        if min_prompt_len == total_len:\n            logits = self.model.forward(tokens, prev_pos)\n            token_logprobs = -F.cross_entropy(\n                input=logits.transpose(1, 2),\n                target=tokens,\n                reduction=\"none\",\n                ignore_index=pad_id,\n            )\n\n        stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n\n        for cur_pos in range(min_prompt_len, total_len):\n            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n            if temperature > 0:\n                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n                next_token = sample_top_p(probs, top_p)\n            else:\n                next_token = torch.argmax(logits[:, -1], dim=-1)\n\n            next_token = next_token.reshape(-1)\n            # only replace token if prompt has already been generated\n            next_token = torch.where(\n                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n            )\n            tokens[:, cur_pos] = next_token\n            if logprobs:\n                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n                    input=logits.transpose(1, 2),\n                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n                    reduction=\"none\",\n                    ignore_index=pad_id,\n                )\n            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n                torch.isin(next_token, stop_tokens)\n            )\n            prev_pos = cur_pos\n            if all(eos_reached):\n                break\n\n        if logprobs:\n            token_logprobs = token_logprobs.tolist()\n        out_tokens, out_logprobs = [], []\n        for i, toks in enumerate(tokens.tolist()):\n            # cut to max gen len\n            start = 0 if echo else len(prompt_tokens[i])\n            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n            probs = None\n            if logprobs:\n                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n            # cut to after eos tok if any\n            for stop_token in self.tokenizer.stop_tokens:\n                try:\n                    eos_idx = toks.index(stop_token)\n                    toks = toks[:eos_idx]\n                    probs = probs[:eos_idx] if logprobs else None\n                except ValueError:\n                    pass\n            out_tokens.append(toks)\n            out_logprobs.append(probs)\n        return (out_tokens, out_logprobs if logprobs else None)\n\n    def text_completion(\n        self,\n        prompts: List[str],\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        max_gen_len: Optional[int] = None,\n        logprobs: bool = False,\n        echo: bool = False,\n    ) -> List[CompletionPrediction]:\n        \"\"\"\n        Perform text completion for a list of prompts using the language generation model.\n\n        Args:\n            prompts (List[str]): List of text prompts for completion.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n                If not provided, it's set to the model's maximum sequence length minus 1.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\n        Returns:\n            List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n\n        Note:\n            This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n            If logprobs is True, token log probabilities are computed for each generated token.\n\n        \"\"\"\n        if max_gen_len is None:\n            max_gen_len = MAX_SEQ_LEN - 1\n        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n        generation_tokens, generation_logprobs = self.generate(\n            prompt_tokens=prompt_tokens,\n            max_gen_len=max_gen_len,\n            temperature=temperature,\n            top_p=top_p,\n            logprobs=logprobs,\n            echo=echo,\n        )\n        if logprobs:\n            return [\n                {\n                    \"generation\": self.tokenizer.decode(t),\n                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n                    \"logprobs\": logprobs_i,\n                }\n                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n            ]\n        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n\n    def chat_completion(\n        self,\n        dialogs: List[Dialog],\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        max_gen_len: Optional[int] = None,\n        logprobs: bool = False,\n    ) -> List[ChatPrediction]:\n        \"\"\"\n        Generate assistant responses for a list of conversational dialogs using the language generation model.\n\n        Args:\n            dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n                If not provided, it's set to the model's maximum sequence length minus 1.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n\n        Returns:\n            List[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n\n        Note:\n            This method generates assistant responses for the provided conversational dialogs.\n            It employs nucleus sampling to introduce controlled randomness in text generation.\n            If logprobs is True, token log probabilities are computed for each generated token.\n        \"\"\"\n        if max_gen_len is None:\n            max_gen_len = MAX_SEQ_LEN - 1\n\n        prompt_tokens = [\n            self.formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n        ]\n        generation_tokens, generation_logprobs = self.generate(\n            prompt_tokens=prompt_tokens,\n            max_gen_len=max_gen_len,\n            temperature=temperature,\n            top_p=top_p,\n            logprobs=logprobs,\n        )\n        if logprobs:\n            return [\n                {\n                    \"generation\": {\n                        \"role\": \"assistant\",\n                        \"content\": self.tokenizer.decode(t),\n                    },\n                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n                    \"logprobs\": logprobs_i,\n                }\n                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n            ]\n        return [\n            {\n                \"generation\": {\n                    \"role\": \"assistant\",\n                    \"content\": self.tokenizer.decode(t),\n                },\n            }\n            for t in generation_tokens\n        ]\n\n\ndef sample_top_p(probs, p):\n    \"\"\"\n    Perform top-p (nucleus) sampling on a probability distribution.\n\n    Args:\n        probs (torch.Tensor): Probability distribution tensor.\n        p (float): Probability threshold for top-p sampling.\n\n    Returns:\n        torch.Tensor: Sampled token indices.\n\n    Note:\n        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n    \"\"\"\n    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    mask = probs_sum - probs_sort > p\n    probs_sort[mask] = 0.0\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    next_token = torch.multinomial(probs_sort, num_samples=1)\n    next_token = torch.gather(probs_idx, -1, next_token)\n    return next_token","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:13:06.002641Z","iopub.execute_input":"2024-09-14T11:13:06.002979Z","iopub.status.idle":"2024-09-14T11:13:06.169956Z","shell.execute_reply.started":"2024-09-14T11:13:06.002944Z","shell.execute_reply":"2024-09-14T11:13:06.168958Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"ckpt_dir = \"/kaggle/input/llama3/\"\ntokenizer_path = \"/kaggle/input/llama3/tokenizer.model\"\n        \ntemperature = 0.6\ntop_p = 0.9\nmax_seq_len = 128\nmax_gen_len = 64\nmax_batch_size = 4\n\ngenerator = Llama.build(\n    ckpt_dir=ckpt_dir,\n    tokenizer_path=tokenizer_path,\n    max_seq_len=max_seq_len,\n    max_batch_size=max_batch_size,\n    model_parallel_size=1\n)\n\nprompts = [\n    # For these prompts, the expected answer is the natural continuation of the prompt\n    \"I believe the meaning of life is\",\n    \"Simply put, the theory of relativity states that \",\n]\nresults = generator.text_completion(\n    prompts,\n    max_gen_len=max_gen_len,\n    temperature=temperature,\n    top_p=top_p,\n)\nfor prompt, result in zip(prompts, results):\n    print(prompt)\n    print(f\"> {result['generation']}\")\n    print(\"\\n==================================\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T11:15:54.328545Z","iopub.execute_input":"2024-09-14T11:15:54.329480Z","iopub.status.idle":"2024-09-14T11:18:45.597323Z","shell.execute_reply.started":"2024-09-14T11:15:54.329441Z","shell.execute_reply":"2024-09-14T11:18:45.596318Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1760461531.py:303: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(ckpt_dir+\"consolidated.00.pth\", map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"PARAMETERS: 8030261248\nLoaded in 150.07 seconds\nI believe the meaning of life is\n>  to be happy. We all want to be happy. We all want to be fulfilled. And we all want to be content. I believe that being happy is the most important thing in life. We all have different ways of achieving happiness. But there is no one way to be happy. It is different for everyone.\n\n==================================\n\nSimply put, the theory of relativity states that \n> 1) the laws of physics are the same for all non-accelerating observers and 2) the speed of light in a vacuum is the same for all observers, regardless of the motion of the light source. This is the most well-known and widely accepted theory of modern physics. Einstein’s theory of relativity\n\n==================================\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}